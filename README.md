## pretrain_BERT_huggingface

An easy demo of pretraining BERT on wikitext-v2 dataset.

### Usage:

1. Prepare the environment:

```
conda env create -f environment.yml
```

2. Start pretraining:

```
python train.py
```

